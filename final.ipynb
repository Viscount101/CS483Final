{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a302ffd-068e-46eb-af8c-dd734aabb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5995d525-9498-421a-b371-8d7e8b87f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
      "0        0001_01     Europa     False     B/0/P    TRAPPIST-1e  39.0  False   \n",
      "1        0002_01      Earth     False     F/0/S    TRAPPIST-1e  24.0  False   \n",
      "2        0003_01     Europa     False     A/0/S    TRAPPIST-1e  58.0   True   \n",
      "3        0003_02     Europa     False     A/0/S    TRAPPIST-1e  33.0  False   \n",
      "4        0004_01      Earth     False     F/1/S    TRAPPIST-1e  16.0  False   \n",
      "...          ...        ...       ...       ...            ...   ...    ...   \n",
      "8688     9276_01     Europa     False    A/98/P    55 Cancri e  41.0   True   \n",
      "8689     9278_01      Earth      True  G/1499/S  PSO J318.5-22  18.0  False   \n",
      "8690     9279_01      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False   \n",
      "8691     9280_01     Europa     False   E/608/S    55 Cancri e  32.0  False   \n",
      "8692     9280_02     Europa     False   E/608/S    TRAPPIST-1e  44.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \n",
      "0             0.0        0.0           0.0     0.0     0.0    Maham Ofracculy  \n",
      "1           109.0        9.0          25.0   549.0    44.0       Juanna Vines  \n",
      "2            43.0     3576.0           0.0  6715.0    49.0      Altark Susent  \n",
      "3             0.0     1283.0         371.0  3329.0   193.0       Solam Susent  \n",
      "4           303.0       70.0         151.0   565.0     2.0  Willy Santantines  \n",
      "...           ...        ...           ...     ...     ...                ...  \n",
      "8688          0.0     6819.0           0.0  1643.0    74.0  Gravior Noxnuther  \n",
      "8689          0.0        0.0           0.0     0.0     0.0    Kurta Mondalley  \n",
      "8690          0.0        0.0        1872.0     1.0     0.0       Fayey Connon  \n",
      "8691          0.0     1049.0           0.0   353.0  3235.0   Celeon Hontichre  \n",
      "8692        126.0     4688.0           0.0     0.0    12.0   Propsh Hontichre  \n",
      "\n",
      "[8693 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "train = data\n",
    "train_xs = train.drop(columns = \"Transported\")\n",
    "train_ys = train['Transported']\n",
    "test_xs = pd.read_csv('test.csv')\n",
    "train_xs.dtypes\n",
    "print(train_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650f44a-2e4f-4967-8c7c-a1e1a6dab899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class OneHotEncodeCategorical(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #print(temp)\n",
    "        columns_to_drop = [\"Name\",\"PassengerId\"]\n",
    "        temp = X.drop(columns = columns_to_drop)\n",
    "        temp.fillna(value=0,inplace=True)\n",
    "\n",
    "        X_copy = temp.copy()\n",
    "        cabin_info = temp['Cabin'].str.extract(r'(?P<Deck>[A-Za-z])/(?P<Number>\\d+)/(?P<Side>[PS])')\n",
    "        #adding deck and number give a nan warning for test scores\n",
    "        #X_copy = pd.concat([X_copy, cabin_info['Deck']], axis=1)\n",
    "        #X_copy = pd.concat([X_copy, cabin_info['Number']], axis=1)\n",
    "\n",
    "        X_copy = pd.concat([X_copy, cabin_info['Side']], axis=1)\n",
    "        X_copy = X_copy.drop(columns=\"Cabin\")  \n",
    "        temp = X_copy;\n",
    "\n",
    "        categorical_columns = temp.select_dtypes(include=['object']).columns\n",
    "        X_encoded = pd.get_dummies(temp, columns=categorical_columns)\n",
    "        print(X_encoded.columns.tolist())\n",
    "\n",
    "        return X_encoded\n",
    "\n",
    "\n",
    "gradientboosting_pipeline = Pipeline([\n",
    "    ('ordinal_encoder', OneHotEncodeCategorical()),\n",
    "    ('scaler',MinMaxScaler()),\n",
    "    ('gradient_boosting', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "gradientboosting_grid = {\n",
    "    'gradient_boosting__subsample': [0.5,0.6,0.7,0.8,0.9,1], #The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting\n",
    "    'gradient_boosting__learning_rate': [0.1,0.3,0.5],  #learning rate shrinks the contribution of each tree\n",
    "    'gradient_boosting__n_estimators': [50,60,70,80,90,100],  #number of boosting stages, larger number tends to do better\n",
    "    'gradient_boosting__max_depth': [3,4,5,6,7],  #limits number of nodes in tree\n",
    "}\n",
    "gradientboosting_search = GridSearchCV(gradientboosting_pipeline, gradientboosting_grid, scoring='accuracy', n_jobs=-1)\n",
    "gradientboosting_search.fit(train_xs, train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d4af79-cec2-4d30-b4ec-a646ef7c775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientboosting_params = gradientboosting_search.best_params_\n",
    "gradientboosting_score = gradientboosting_search.best_score_\n",
    "print(f\"Accuracy: {gradientboosting_score}\")\n",
    "print(f\"Best params: {gradientboosting_params}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec9324-ec9c-49ef-8c6d-41e59c15554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gradientboosting = gradientboosting_search.best_estimator_\n",
    "\n",
    "predicted_values = best_gradientboosting.predict(test_xs)\n",
    "passenger_ids = test_xs['PassengerId'].reset_index(drop=True)\n",
    "\n",
    "result_df = pd.DataFrame({'PassengerId': passenger_ids, 'Transported': predicted_values})\n",
    "\n",
    "print(result_df)\n",
    "result_df.to_csv('predicted_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b72fe4-84a3-489d-9972-b9da09b77da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
